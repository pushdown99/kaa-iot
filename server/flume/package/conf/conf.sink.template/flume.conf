#
# Copyright 2014-2016 CyberVision, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# The configuration file needs to define the sources,.
# the channels and the sinks.
# Sources, channels and sinks are defined per agent,.
# in this case called 'kaa-sink'

kaa-sink.sources = avroSource
kaa-sink.channels = fileChannel
kaa-sink.sinks = kaaHdfSink

# The component type name 'avro'
# Avro Source. Listens on Avro port and receives events from external Avro client streams.
kaa-sink.sources.avroSource.type = avro

# Reference to the channel for storing events
kaa-sink.sources.avroSource.channels = fileChannel

# hostname or IP address to listen on
kaa-sink.sources.avroSource.bind = ${kaa-sinkHostName}

# Port # to bind to
kaa-sink.sources.avroSource.port = ${kaa-sinkPort}

# Maximum number of worker threads to spawn
kaa-sink.sources.avroSource.threads = 20

# The component type name 'file'. Use File Channel 
kaa-sink.channels.fileChannel.type = file

# Maximum capacity of the channel
kaa-sink.channels.fileChannel.capacity = 100000000

# he maximum size of transaction supported by the channel
kaa-sink.channels.fileChannel.transactionCapacity = 10000000

# Amount of time (in sec) to wait for a put operation
kaa-sink.channels.fileChannel.keep-alive = 1

# Amount of time (in sec) to wait for a write operation
kaa-sink.channels.fileChannel.write-timeout = 30

# The directory where checkpoint file will be stored
kaa-sink.channels.fileChannel.checkpointDir = /flume/kaa-sink/file-channel/checkpoint

# Comma separated list of directories for storing log files. 
# Using multiple directories on separate disks can improve file channel peformance
kaa-sink.channels.fileChannel.dataDirs = /flume/kaa-sink/file-channel/data

# A custom implementation of the Sink interface.
# KaaHdfsSink is extended version of regular Flume HDFS Sink.
# It has more specific configuration to distribute log events among HDFS using information from headers.
# And also it has a better performance compared with the regular HDFS Sink by using dynamic cache of HDFS writers.
kaa-sink.sinks.kaaHdfSink.type = org.kaaproject.kaa.server.flume.sink.hdfs.KaaHdfsSink

# Reference to the channel to read events
kaa-sink.sinks.kaaHdfSink.channel = fileChannel

# HDFS directory path (eg hdfs://namenode/flume/webdata/)
kaa-sink.sinks.kaaHdfSink.rootHdfsPath = hdfs://${nameNode}/${hdfsRoot}

# Max events per transaction
kaa-sink.sinks.kaaHdfSink.hdfs.txnEventMax = ${eventsPerTransaction}

# Number of threads per HDFS sink for HDFS IO ops (open, write, etc.)
kaa-sink.sinks.kaaHdfSink.hdfs.threadsPoolSize = 20

# Number of threads per HDFS sink for scheduling timed file rolling
kaa-sink.sinks.kaaHdfSink.hdfs.rollTimerPoolSize = 1

# Allow only this number of open files. If this number is exceeded, the oldest file is closed.
kaa-sink.sinks.kaaHdfSink.hdfs.maxOpenFiles = 500

# Interval (in sec) to perform HDFS writers cache cleanup to release resources 
kaa-sink.sinks.kaaHdfSink.hdfs.cacheCleanupInterval = 600

# Max HDFS writer idle timeout (in sec) after which writer get closed and removed from cache
kaa-sink.sinks.kaaHdfSink.hdfs.writerExpirationInterval = 3600

# Number of milliseconds allowed for HDFS operations, such as open, write, flush, close. 
# This number should be increased if many HDFS timeout operations are occurring.
kaa-sink.sinks.kaaHdfSink.hdfs.callTimeout = 10000

# Number of seconds to wait before rolling current file (0 = never roll based on time interval)
kaa-sink.sinks.kaaHdfSink.hdfs.rollInterval = 86400000

# File size to trigger roll, in bytes (0: never roll based on file size)
kaa-sink.sinks.kaaHdfSink.hdfs.rollSize = 0

# Number of events written to file before it rolled (0 = never roll based on number of events)
kaa-sink.sinks.kaaHdfSink.hdfs.rollCount = 5500000

# Number of events written to file before it is flushed to HDFS
kaa-sink.sinks.kaaHdfSink.hdfs.batchSize = 10000

# Default block size of the HDFS files written by flume
kaa-sink.sinks.kaaHdfSink.hdfs.default.blockSize = 2097152

# Name prefixed to files created by Flume in hdfs directory
kaa-sink.sinks.kaaHdfSink.hdfs.filePrefix = data

# Interval (in sec) to perform logging of current processing statistics
kaa-sink.sinks.kaaHdfSink.statisticsInterval = 60

# Compression codec used by Kaa Avro Event Serializer to compress avro records (default 'null' - no compression) 
kaa-sink.sinks.kaaHdfSink.serializer.compressionCodec = snappy

# Avro schema source type to obtain avro schema for log events by application token and schema version.
# Can be either 'rest' or 'local'. 'rest' - use Kaa REST API to obtain avro schema. 'local' - read schema from files stored 
# in local filesystem
kaa-sink.sinks.kaaHdfSink.serializer.avro.schema.source = ${avroSchemaSource}

# Kaa Admin Rest API host
kaa-sink.sinks.kaaHdfSink.serializer.kaa.rest.host = ${kaaRestHost}

# Kaa Admin Rest API port
kaa-sink.sinks.kaaHdfSink.serializer.kaa.rest.port = ${kaaRestPort}

# Kaa Admin User. User should have 'Tenant developer' or 'Tenant user' authority.
kaa-sink.sinks.kaaHdfSink.serializer.kaa.rest.user = ${kaaRestUser}

# Kaa Admin User Password.
kaa-sink.sinks.kaaHdfSink.serializer.kaa.rest.password = ${kaaRestPassword}

# Absolute path to directory with schema files (used for 'local' avro schema source)
kaa-sink.sinks.kaaHdfSink.serializer.avro.schema.local.root = ${avroSchemaLocalRoot}

# Kerberos user principal for accessing secure HDFS
kaa-sink.sinks.kaaHdfSink.hdfs.kerberosPrincipal = flume/_HOST@LOCALHOST

# Kerberos keytab for accessing secure HDFS
kaa-sink.sinks.kaaHdfSink.hdfs.kerberosKeytab = /etc/flume-ng/conf/flume.keytab
